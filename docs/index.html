<!DOCTYPE html>
<!-- saved from url=(0024)http://chahuja.com/pats/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">

<title>PATS Dataset (Pose, Audio, Transcript, Style) | PATS Dataset</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="PATS Dataset (Pose, Audio, Transcript, Style)">
<meta property="og:locale" content="en_US">
<meta name="description" content="Pose-Audio-Transcript-Style Dataset">
<meta property="og:description" content="Pose-Audio-Transcript-Style Dataset">
<link rel="canonical" href="http://chahuja.com/pats/">
<meta property="og:url" content="http://chahuja.com/pats/">
<meta property="og:site_name" content="PATS Dataset">
<script type="application/ld+json">
{"@type":"WebSite","url":"http://chahuja.com/pats/","headline":"PATS Dataset (Pose, Audio, Transcript, Style)","description":"<strong>P</strong>ose, <strong>A</strong>udio, <strong>T</strong>ranscript, <strong>S</strong>tyle","name":"PATS Dataset","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="./PATS Dataset (Pose, Audio, Transcript, Style) _ PATS Dataset_files/style.css">
  </head>
  <body>
    <section class="page-header">
      <img src="https://user-images.githubusercontent.com/43928520/90432137-1cbcaf80-e098-11ea-8491-0f7c92da4b29.png" width="100" height="100">
      <h1 class="project-name">PATS Dataset </h1>
      <h2 class="project-tagline"><strong>P</strong>ose, <strong>A</strong>udio, <strong>T</strong>ranscript, <strong>S</strong>tyle</h2>
      
        <a href="https://github.com/chahuja/pats" class="btn">View Dataset Repository</a>
      
      
    </section>

    <section class="main-content">
       
<h2 id="intro">Why PATS?</h2>
<p>PATS was developed to address audio and text siganls in the task of pose generation. Utilizing both signals would allow us to generate relevant and expressive gestures 
 useful for human computer interaction.</p>
       
<h2 id="tasks">Tasks</h2>
 The PATS dataset allows us to explore the following tasks:

<h3 id="CMT">Cross-Modal Translation</h3>
  The three modalities (Pose, Audio, Transcriptions), which are inherently very different from one another, offers users with an unique task of translation between very different modalities.
  <li>Audio to Pose</li>
  <li>Transcript to Pose</li>
  <li>Audio + Transcript to Pose</li>     

       
<h3 id="ST">Style Transfer</h3>
  The data also poses and interesting task of transferring Style. As pose styles and language styles are often idiosyncratic, PATS offers a unique dataset to perform research on style transfer of various modalities.
  <li>Gesture Style Transfer</li>
  <li>Language Style Transfer</li>
       
<h3 id="GT">Grounding </h3>
   An important question is learning the pairing of multiple modalities: grounding. With aligned pose audio and transcripts, PATS provides a good platform for conducting research for multimodal grounding of conversational language to hand gestures.
       
<h2 id="details">What can you find in this dataset?</h2>
 
<div>       
  <ul style="float: left; min-width: 500px; border-right: solid 1px;">
  <li>Contains transcribed <strong>Pose</strong> data with aligned <strong>Audio</strong> and <strong>Transcriptions</strong>
    <ul>
      <li>25 Speakers with different <strong>Styles</strong></li>
      <li>Includes 10 speakers from <a href="https://people.eecs.berkeley.edu/~shiry/projects/speech2gesture/index.html">Ginosar, et al. (CVPR 2019)</a></li>
      <li>15 talk show hosts, 5 lecturers, 3 YouTubers, and 2 televangelists</li>
    </ul>
  </li>
  <li>251 hours of data 
    <ul>
      <li>Around ~ 84000 intervals</li> 
      <li>Mean: 10.7s per interval</li>
      <li>Standard Deviation: 13.5s per interval<br></li>
    </ul>
  </li>
</ul>
<img src="https://user-images.githubusercontent.com/43928520/90454983-c022ba00-e0c2-11ea-991e-36bd5cb3b38b.png" width="400px" border="1px" style="float: left;" />
       </div>
       


<br>
       
       


<h2 id="dataset-features">Dataset Features</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Features</th>
      <th style="text-align: left">Available Representations</th>
      <th style="text-align: left">Collection Methodology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Audio</td>
      <td style="text-align: left">Log-mel Spectrograms</td>
      <td style="text-align: left">Audio scraped from Youtube</td>
    </tr>
    <tr>
      <td rowspan = "3">Transcripts</td>
      <td> Word Tokens </td>
      <td>Transcribed using <a href="https://cloud.google.com/speech-to-text">Google ASR</a> with an estimated word error rate of 0.29* (*-estimated on available transcripts for a subset of videos)
         </td>
    </tr>
     
    <tr>
      <td> Bert Embeddings</td>
      <td style="text-align: left"> Pre-trained model 'bert_base_uncased' from <a href="https://huggingface.co/transformers/model_doc/bert.html">HuggingFace</a>,
         based on <a href="https://arxiv.org/abs/1810.04805"> Devlin, et al. (NAACL 2019)</a>
          </td>
    </tr>
     
    <tr>
      <td> Word2Vec Embeddings </td>
      <td style="text-align: left">  <a href="https://code.google.com/archive/p/word2vec/"> Word2Vec</a>  based on <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al. (NIPS 2013)</a> </td>
    </tr>
     
    <tr>
      <td style="text-align: left">Pose</td>
      <td style="text-align: left">2D Skeletal Keypoints</td>
      <td style="text-align: left">Processed using <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a></td>
    </tr>
  </tbody>
</table>

<iframe width="560" height="315" src="https://www.youtube.com/watch?v=BQf1e-K_oek" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
       
       
<h3 id="dataset_navi">Navigating Through Speakers</h3>
<h4><a id="user-content-header-1" class="anchor" href="#header-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>
       Speakers' clustering based on transcripts
       </h4>
<p><img src="https://user-images.githubusercontent.com/43928520/90695572-b925b400-e248-11ea-9c69-e391fd3f80dc.png" width="1000">
       </p>

       
<h4><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>
       Speakers' Lexical Diversity and Pose Expressivity (Spatial Extent Average)
       </h4>
<p><img src="https://user-images.githubusercontent.com/43928520/90699627-c5fad580-e251-11ea-93f9-6d2004afdf0b.png" width="600">
       </p>

       

<h3 id="reference">Reference</h3>
<p>If you found this dataset helpful, please cite the following paper:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight">
<code>
@inproceedings{ahuja2020style,
    title={Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach},
    author={Chaitanya Ahuja and Dong Won Lee and Yukiko I. Nakano and Louis-Philippe Morency},
    venue = {European Conference on Computer Vision (ECCV)}
    year={2020},
    month = {August},
    year = {2020},
    url={https://arxiv.org/abs/2007.12553}
}
</code></pre></div></div>


      <footer class="site-footer">

         
      <h3 id="Author">Author</h3>
      <p>If you found this dataset helpful, please cite the following paper:</p>
  
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  

</body></html>
