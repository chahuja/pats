<!DOCTYPE html>
<!-- saved from url=(0024)http://chahuja.com/pats/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" type="text/css" href="css/normalize.css" media="screen">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="css/stylesheet.css" media="screen">
  <link rel="stylesheet" type="text/css" href="css/github-light.css" media="screen">
  <link href="assets/css/style2.css" rel="stylesheet">
  <link href="assets/css/style.css" rel="stylesheet">
</head>
<body style="background-color: white;">
  
  <title>PATS Dataset (Pose, Audio, Transcript, Style) | PATS Dataset</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="PATS Dataset (Pose, Audio, Transcript, Style)">
  <meta property="og:locale" content="en_US">
  <meta name="description" content="Pose-Audio-Transcript-Style Dataset">
  <meta property="og:description" content="Pose-Audio-Transcript-Style Dataset">
  <link rel="canonical" href="http://chahuja.com/pats/">
  <meta property="og:url" content="http://chahuja.com/pats/">
  <meta property="og:site_name" content="PATS Dataset">
  <script type="application/ld+json">
    {"@type":"WebSite","url":"http://chahuja.com/pats/","headline":"PATS Dataset (Pose, Audio, Transcript, Style)","description":"<strong>P</strong>ose, <strong>A</strong>udio, <strong>T</strong>ranscript, <strong>S</strong>tyle","name":"PATS Dataset","@context":"https://schema.org"}</script>
    <!-- End Jekyll SEO tag -->
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
  </head>
  <body>
    <section class="page-header">
      <img src="figs/pats_logo.png" width="100">
      
      
      <h1 class="project-name">PATS Dataset </h1>
      <h2 class="project-tagline"><strong>P</strong>ose, <strong>A</strong>udio, <strong>T</strong>ranscript, <strong>S</strong>tyle</h2>
      
      <a href="http://chahuja.com/pats/download.html"class="button button2">Download</a> <a href="https://github.com/chahuja/pats" class="button button2">Dataset Scripts</a>
      
      
    </section>
    
    <section class="main-content">
      
      <h1 id="intro">Why PATS?</h1>
      <p>PATS was collected to study correlation of co-speech gestures with audio and text signals. The dataset consists of a <b>diverse</b> and <b>large amount</b> of aligned <b>pose, audio and transcripts</b>. 
        With this dataset, we hope to provide a benchmark which would help develop technologies for virtual agents which generate natural and relevant gestures. </br> </p>
        <center> 
          <iframe width="560" height="315" src="https://www.youtube.com//embed/BQf1e-K_oek?autoplay=1&mute=1&loop=1&playlist=BQf1e-K_oek" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen float: "center"></iframe>
        </center>       
        
        <h1 id="details">What can you find in this dataset?</h1>
        <div>       
          <ul style="float: left; max-width: 480px">
            <li>Transcribed <strong>Pose</strong> data with aligned <strong>Audio</strong> and <strong>Transcriptions</strong>
              <ul>
                <li>25 Speakers with different <strong>Styles</strong></li>
                <li>Includes 10 speakers from <a href="https://people.eecs.berkeley.edu/~shiry/projects/speech2gesture/index.html">Ginosar, et al. (CVPR 2019)</a></li>
                <li>15 talk show hosts, 5 lecturers, 3 YouTubers, and 2 televangelists</li>
              </ul>
            </li>
            <li>251 hours of data 
              <ul>
                <li>Around ~ 84000 intervals</li> 
                <li>Mean: 10.7s per interval</li>
                <li>Standard Deviation: 13.5s per interval<br></li>
              </ul>
            </li>
          </ul>
          <img src="https://user-images.githubusercontent.com/43928520/90454983-c022ba00-e0c2-11ea-991e-36bd5cb3b38b.png" width = "360px" border="px" style="float: center;" />
        </div>
        
        <h1 id="tasks">Tasks</h1>
        Three modalities -i.e. Pose, Audio, Transcriptions- in many Styles available in the PATS dataset present a unique opportunity for the following tasks,
        
        <!-- <h3 id="CMT">Cross-Modal Translation</h3>
          The three modalities (Pose, Audio, Transcriptions), present in the dataset, offer users with an unique task of translation between each other.
          <li>Audio to Pose</li>
          <li>Transcript to Pose</li>
          <li>Audio + Transcript to Pose</li>     
          
          
          <h3 id="ST">Style Transfer</h3>
          The data also poses and interesting task of transferring Style and Language. As pose styles and language styles are often idiosyncratic to each speaker, PATS offers a unique dataset to perform research on style transfer of various modalities.
          <li>Gesture Style Transfer</li>
          <li>Language Style Transfer</li>
          
          <h3 id="GT">Grounding </h3>
          An important question is learning the pairing of multiple modalities: grounding. With aligned pose audio and transcripts, PATS provides a good platform for conducting research for multimodal grounding of conversational language to hand gestures. -->
          
          <table>
            <thead>
              <tr>
                <th style="text-align: center">Cross-Modal Translation</th>
                <th style="text-align: center">Style Transfer</th>
                <th style="text-align: center">Grounding</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: left" width="320px"> Pose Audio and Transcripts are aligned making PATS a good test-bench for cross-modal translation tasks,
                  <li>Audio to Pose</li>
                  <li>Transcript to Pose</li>
                  <li>Audio + Transcript to Pose</li></td>  
                  
                  
                  <td style="text-align: left" width="320px">As pose styles and language styles are often idiosyncratic to each speaker, PATS offers a unique dataset to perform research on style transfer of various modalities.
                    <li>Gesture Style Transfer</li>
                    <li>Language Style Transfer</li></td>
                    
                    <td style="text-align: left" width="320px">An important question is learning the pairing of multiple modalities: grounding. With aligned pose audio and transcripts, PATS provides a good platform for conducting research for multimodal grounding of conversational language to hand gestures.</td>
                  </tr>
                </tbody>
              </table>
              <h1 id="Features">Features</h1>
              <table>
                <thead>
                  <tr>
                    <th style="text-align: left">Features</th>
                    <th style="text-align: left">Available Representations</th>
                    <th style="text-align: left">Collection Methodology</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="text-align: left">Audio</td>
                    <td style="text-align: left">Log-mel Spectrograms</td>
                    <td style="text-align: left">Audio scraped from Youtube</td>
                  </tr>
                  <tr>
                    <td rowspan = "3">Transcripts</td>
                    <td> Word Tokens </td>
                    <td>Transcribed using <a href="https://cloud.google.com/speech-to-text">Google ASR</a> with an estimated word error rate of 0.29* (*-estimated on available transcripts for a subset of videos)
                    </td>
                  </tr>
                  
                  <tr>
                    <td> Bert Embeddings</td>
                    <td style="text-align: left"> Pre-trained model 'bert_base_uncased' from <a href="https://huggingface.co/transformers/model_doc/bert.html">HuggingFace</a>,
                      based on <a href="https://arxiv.org/abs/1810.04805"> Devlin, et al. (NAACL 2019)</a>
                    </td>
                  </tr>
                  
                  <tr>
                    <td> Word2Vec Embeddings </td>
                    <td style="text-align: left">  <a href="https://code.google.com/archive/p/word2vec/"> Word2Vec</a>  based on <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al. (NIPS 2013)</a> </td>
                  </tr>
                  
                  <tr>
                    <td style="text-align: left">Pose</td>
                    <td style="text-align: left">2D Skeletal Keypoints</td>
                    <td style="text-align: left">Processed using <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a></td>
                  </tr>
                </tbody>
              </table>
              
              
              <h1 id="dataset_navi">Navigating Through Speakers</h1>
              
              <p>
                The speakers in PATS have diverse lexical content in their transcripts along with diverse gestures. 
                The following graphs will help you navigate through the speakers in the dataset, 
                should you want to work with specific speakers with a different gesture and/or lexical diversities. </p> 
                
                <p>Fig 1 shows speakers clustered hierarchically based on the content of their transcripts and Fig 2 shows each speaker's position on a 
                  lexical diversity vs spatial extent plot. </p> 
                  
                  <p> As shown in Fig. 1, speakers in the same domain (i.e. TV Show Hosts) share similar language, as demonstrated in the clusters. 
                    Furthermore, in Fig. 2, we can see that TV show speakers are generally more expressive with their hands and and words while Televangelists are less so. 
                    Speakers on the top right corner of Fig 2, are more challenging to model in the task of gesture generation as they have a greater diversity of vocabulary
                    as well as gestures.
                  </p>
                  
                  <img src="https://user-images.githubusercontent.com/43928520/90968972-ea101e00-e4c0-11ea-81dc-3e0fd25a8252.png" width="1000">
                  
                  
                  
                  <h1 id="reference">Reference(s)</h1>
                  <p>If you found this dataset helpful, please consider citing the following paper(s):</p>
                  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight">
                    <ol>
                    <li><b>No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures</b><br><i>EMNLP Findings 2020</i> - <a href="http://github.com/chahuja/aisle">[code]</a>                    
<code>@inproceedings{ahuja2020no,
  title={No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures},
  author={Ahuja, Chaitanya and Lee, Dong Won and Ishii, Ryo and Morency, Louis-Philippe},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={1884--1895},
  year={2020}
}</code>
</li>
                      <li><b>Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach</b><br><i>ECCV 2020</i> - <a href="http://chahuja.com/mix-stage">[website]</a><a href="http://github.com/chahuja/mix-stage">[code]</a>                    
<code>@inproceedings{ahuja2020style,
  title={Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach},
  author={Chaitanya Ahuja and Dong Won Lee and Yukiko I. Nakano and Louis-Philippe Morency},
  venue = {European Conference on Computer Vision (ECCV)}
  year={2020},
  month = {August},
  year = {2020},
  url={https://arxiv.org/abs/2007.12553}
}</code>
</li>
<li><b>Learning individual styles of conversational gesture</b><br>CVPR 2019 - <a href="http://people.eecs.berkeley.edu/~shiry/projects/speech2gesture/">[website]</a>
<code>@inproceedings{ginosar2019learning,
  title={Learning individual styles of conversational gesture},
  author={Ginosar, Shiry and Bar, Amir and Kohavi, Gefen and Chan, Caroline and Owens, Andrew and Malik, Jitendra},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3497--3506},
  year={2019}
}</code>
We kindly ask you to cite Ginosar et. al. as well, whose 10 speakers' pose and audio files are used for our dataset.
</li>
</ol>
</pre></div></div>

                    
                    
                    
                    <h1 id="Authors">Authors</h1>
                    <table>
                      <tbody>
                        <tr style="display: flex; flex-wrap: wrap;">
                          <td style="text-align: left; border:0px">
                            <a href="http://chahuja.com">
                            <div class="authors">
                              <img src="figs/chahuja.jpg" width="200px"><br>
                              Chaitanya Ahuja
                            </div>
                          </a>
                          </td>
                          <td style="text-align: left; border:0px">
                            <a href="javascript: void(0)">
                            <div class="authors">
                              <img src="figs/dong.jpg" width="200px"><br>
                              Dong Won Lee
                            </div>
                          </a>
                          </td>
                          <td style="text-align: left; border:0px">
                            <a href="http://www.ci.seikei.ac.jp/nakano/index_e.html">
                            <div class="authors">
                              <img src="figs/yukiko.jpg" width="200px"><br>
                              Yukiko Nakano
                            </div>
                          </a>
                          </td>
                          <td style="text-align: left; border:0px">
                            <a href="https://www.cs.cmu.edu/~morency/">
                            <div class="authors">
                              <img src="figs/lp.jpg" width="200px"><br>
                              Louis-Philippe Morency
                            </div>
                          </a>
                          </td>
                        </tr>
                      </tbody>
                    </table>
                      
                      <footer class="site-footer">
                        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub Pages</a>.</span>
                      </footer>
                    </section>
                    
                    
                    
                    
                  </body></html>
                  
