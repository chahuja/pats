<!DOCTYPE html>
<!-- saved from url=(0024)http://chahuja.com/pats/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">

<title>PATS Dataset (Pose, Audio, Transcript, Style) | PATS Dataset</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="PATS Dataset (Pose, Audio, Transcript, Style)">
<meta property="og:locale" content="en_US">
<meta name="description" content="Pose-Audio-Transcript-Style Dataset">
<meta property="og:description" content="Pose-Audio-Transcript-Style Dataset">
<link rel="canonical" href="http://chahuja.com/pats/">
<meta property="og:url" content="http://chahuja.com/pats/">
<meta property="og:site_name" content="PATS Dataset">
<script type="application/ld+json">
{"@type":"WebSite","url":"http://chahuja.com/pats/","headline":"PATS Dataset (Pose, Audio, Transcript, Style)","description":"<strong>P</strong>ose, <strong>A</strong>udio, <strong>T</strong>ranscript, <strong>S</strong>tyle","name":"PATS Dataset","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="./PATS Dataset (Pose, Audio, Transcript, Style) _ PATS Dataset_files/style.css">
  </head>
  <body>
    <section class="page-header">
      <img src="https://user-images.githubusercontent.com/43928520/90432137-1cbcaf80-e098-11ea-8491-0f7c92da4b29.png" width="100" height="100">
      <h1 class="project-name">PATS Dataset </h1>
      <h2 class="project-tagline"><strong>P</strong>ose, <strong>A</strong>udio, <strong>T</strong>ranscript, <strong>S</strong>tyle</h2>
      
        <a href="https://github.com/chahuja/pats" class="btn">View Dataset Repository</a>
      
      
    </section>

    <section class="main-content">

<ul>
  <li>Contains transcribed <strong>Pose</strong> data with aligned <strong>Audio</strong> and <strong>Transcriptions</strong>
    <ul>
      <li>25 Speakers with different <strong>Styles</strong></li>
      <li>Includes 10 speakers from <a href="https://people.eecs.berkeley.edu/~shiry/projects/speech2gesture/index.html">Ginosar, et al. (CVPR 2019)</a></li>
      <li>15 talk show hosts, 5 lecturers, 3 YouTubers, and 2 televangelists</li>
    </ul>
  </li>
  <li>251 hours of data
    <ul>
      <li>Mean: 10.7s per interval</li>
      <li>Standard Deviation: 13.5s per segment<br></li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43928520/90454983-c022ba00-e0c2-11ea-991e-36bd5cb3b38b.png" width="1000"></p>

<h2 id="dataset-features">Dataset Features</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Features</th>
      <th style="text-align: left">Available Representations</th>
      <th style="text-align: left">Collection Methodology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Audio</td>
      <td style="text-align: left">Log-mel Spectrograms</td>
      <td style="text-align: left">Audio directly scraped from Youtube</td>
    </tr>
    <tr>
      <td rowspan = "3">Language</td>
      <td> Word Tokens </td>
      <td> Transcribed by <a href="https://cloud.google.com/speech-to-text">Google ASR</a>, Estimated WER = 0.29 </td>
    </tr>
     
    <tr>
      <td> Bert Embeddings</td>
      <td style="text-align: left">bert_base_uncased from <a href="https://huggingface.co/transformers/model_doc/bert.html">HuggingFace</a>,
         <a href="https://arxiv.org/abs/1810.04805"> Devlin, et al. (NAACL 2019)</a>
          </td>
    </tr>
     
    <tr>
      <td> Word2Vec Embeddings </td>
      <td style="text-align: left"> <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al. (NIPS 2013)</a> </td>
    </tr>
     
    <tr>
      <td style="text-align: left">Pose</td>
      <td style="text-align: left">2D Skeletal Keypoints</td>
      <td style="text-align: left">Used <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a></td>
    </tr>
  </tbody>
</table>

       
<h3><a id="user-content-header-1" class="anchor" href="#header-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>
       Dendrogram of hierchical clustering of speakers’ full transcripts
       </h3>
<p><img src="https://user-images.githubusercontent.com/43928520/90695572-b925b400-e248-11ea-9c69-e391fd3f80dc.png" width="1000">
       </p>

       
<h3><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>
       Speakers' Lexical Diversity vs Expressivity (Spatial Extent Average)
       </h3>
<p><img src="https://user-images.githubusercontent.com/43928520/90699627-c5fad580-e251-11ea-93f9-6d2004afdf0b.png" width="600">
       </p>

       
       
<h2 id="link-to-data">Link to data:</h2>
<p>Data can be downloaded here : <a href="https://google.com">LINK</a><br>
We’ve also included a csv file, in which you can see the video source and the specific section we took the intervals from.</p>

<h2 id="requirements">Requirements:</h2>

<h2 id="tasks">Tasks</h2>
<ul>
  <li>Cross-Modal Translation
    <ul>
      <li>Audio to Pose</li>
      <li>Transcript to Pose</li>
      <li>Audio + Transcript to Pose</li>
    </ul>
  </li>
   
  <li>Style Transfer</li>
   <ul>
      <li>Gesture Style Transfer</li>
      <li>Language Style Transfer</li>
   </ul>
  <li>Grounding Tasks
</ul>       

<h3 id="reference">Reference</h3>
<p>If you found this dataset helpful, please cite the following paper:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{ahuja2020style,
    title={Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach},
    author={Chaitanya Ahuja and Dong Won Lee and Yukiko I. Nakano and Louis-Philippe Morency},
    year={2020},
    eprint={2007.12553},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/chahuja/pats">pats</a> is maintained by <a href="https://github.com/chahuja">chahuja</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  

</body></html>
